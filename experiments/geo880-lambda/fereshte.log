Parameter 's' changed from '' to 'a.vcb'
Parameter 't' changed from '' to 'b.vcb'
Parameter 'c' changed from '' to 'a_b.snt'
Parameter 'p0' changed from '-1' to '0.98'
ERROR: parameter 'coocurrencefile' does not exist.
WARNING: ignoring unrecognized option:  -CoocurrenceFile
ERROR: parameter 'cocooc' does not exist.
WARNING: ignoring unrecognized option:  co.cooc
Parameter 'o' changed from '115-08-25.154825.fereshte' to 'fereshte'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 5  (number of iterations for Model 3)
model4iterations = 5  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 115-08-25.154825.fereshte.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 0  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 0  (1: do not write any files)
o = fereshte  (output file prefix)
onlyaldumps = 0  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = a_b.snt  (training corpus file name)
d =   (dictionary file name)
s = a.vcb  (source vocabulary file name)
t = b.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.2  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 64  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.98  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 5  (number of iterations for Model 3)
model4iterations = 5  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 115-08-25.154825.fereshte.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 0  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 0  (1: do not write any files)
o = fereshte  (output file prefix)
onlyaldumps = 0  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = a_b.snt  (training corpus file name)
d =   (dictionary file name)
s = a.vcb  (source vocabulary file name)
t = b.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.2  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 64  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.98  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:a.vcb
Reading vocabulary file from:b.vcb
Source vocabulary list has 56 unique tokens 
Target vocabulary list has 253 unique tokens 
Calculating vocabulary frequencies from corpus a_b.snt
Reading more sentence pairs into memory ... 
WARNING: The following sentence pair has source/target sentence length ration more than
the maximum allowed limit for a source word fertility
 source length = 1 target length = 11 ratio 11 ferility limit : 9
Shortening sentence 
Sent No: 409 , No. Occurrences: 1
0 41 
12 86 136 14 179 66 160 161 27 160 180 
Corpus fits in memory, corpus has: 849 sentence pairs.
 Train total # sentence pairs (weighted): 849
Size of source portion of the training corpus: 3395 tokens
Size of the target portion of the training corpus: 6444 tokens 
In source portion of the training corpus, only 55 unique tokens appeared
In target portion of the training corpus, only 251 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 6444/(4244-849)== 1.89809
==========================================================
Model1 Training Started at: Tue Aug 25 15:48:25 2015

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 8.65652 PERPLEXITY 403.526
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 10.9593 PERPLEXITY 1991.03
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 5.83741 PERPLEXITY 57.1787
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 7.25629 PERPLEXITY 152.884
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 5.53689 PERPLEXITY 46.4269
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 6.68155 PERPLEXITY 102.647
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 5.40916 PERPLEXITY 42.4931
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 6.37975 PERPLEXITY 83.2714
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 5.34979 PERPLEXITY 40.7799
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 6.20857 PERPLEXITY 73.9545
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 0 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 55  #classes: 52
Read classes: #words: 252  #classes: 52

==========================================================
Hmm Training Started at: Tue Aug 25 15:48:25 2015

-----------
Hmm: Iteration 1
A/D table contains 639 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 5.31926 PERPLEXITY 39.9261
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 6.13225 PERPLEXITY 70.144

Hmm Iteration: 1 took: 0 seconds

-----------
Hmm: Iteration 2
A/D table contains 639 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 5.02463 PERPLEXITY 32.551
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 5.59812 PERPLEXITY 48.4399

Hmm Iteration: 2 took: 0 seconds

-----------
Hmm: Iteration 3
A/D table contains 639 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 4.87358 PERPLEXITY 29.3152
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 5.31774 PERPLEXITY 39.884

Hmm Iteration: 3 took: 0 seconds

-----------
Hmm: Iteration 4
A/D table contains 639 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 4.77641 PERPLEXITY 27.4057
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 5.1355 PERPLEXITY 35.1512

Hmm Iteration: 4 took: 0 seconds

-----------
Hmm: Iteration 5
A/D table contains 639 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 4.711 PERPLEXITY 26.1909
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 5.01349 PERPLEXITY 32.3007

Hmm Iteration: 5 took: 0 seconds

Entire Hmm Training took: 0 seconds
==========================================================
Read classes: #words: 55  #classes: 52
Read classes: #words: 252  #classes: 52
Read classes: #words: 55  #classes: 52
Read classes: #words: 252  #classes: 52

==========================================================
Starting H3333344444:  Viterbi Training
 H3333344444 Training Started at: Tue Aug 25 15:48:25 2015


---------------------
THTo3: Iteration 1
#centers(pre/hillclimbed/real): 1 1 1  #al: 54.9764 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 639 parameters.
A/D table contains 1098 parameters.
NTable contains 560 parameter.
p0_count is 4597.3 and p1 is 918.88; p0 is 0.98 p1: 0.02
THTo3: TRAIN CROSS-ENTROPY 4.08431 PERPLEXITY 16.9629
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 4.25723 PERPLEXITY 19.1229

THTo3 Viterbi Iteration : 1 took: 0 seconds

---------------------
Model3: Iteration 2
#centers(pre/hillclimbed/real): 1 1 1  #al: 55.1449 #alsophisticatedcountcollection: 0 #hcsteps: 1.70554
#peggingImprovements: 0
A/D table contains 639 parameters.
A/D table contains 1100 parameters.
NTable contains 560 parameter.
p0_count is 4936.41 and p1 is 753.796; p0 is 0.98 p1: 0.02
Model3: TRAIN CROSS-ENTROPY 5.66487 PERPLEXITY 50.7335
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 5.82082 PERPLEXITY 56.525

Model3 Viterbi Iteration : 2 took: 0 seconds

---------------------
Model3: Iteration 3
#centers(pre/hillclimbed/real): 1 1 1  #al: 55.0954 #alsophisticatedcountcollection: 0 #hcsteps: 1.7338
#peggingImprovements: 0
A/D table contains 639 parameters.
A/D table contains 1100 parameters.
NTable contains 560 parameter.
p0_count is 5059.86 and p1 is 692.07; p0 is 0.98 p1: 0.02
Model3: TRAIN CROSS-ENTROPY 5.56166 PERPLEXITY 47.2308
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 5.68944 PERPLEXITY 51.6052

Model3 Viterbi Iteration : 3 took: 0 seconds

---------------------
Model3: Iteration 4
#centers(pre/hillclimbed/real): 1 1 1  #al: 55.0907 #alsophisticatedcountcollection: 0 #hcsteps: 1.75029
#peggingImprovements: 0
A/D table contains 639 parameters.
A/D table contains 1100 parameters.
NTable contains 560 parameter.
p0_count is 5130.77 and p1 is 656.613; p0 is 0.98 p1: 0.02
Model3: TRAIN CROSS-ENTROPY 5.52268 PERPLEXITY 45.9719
Model3: (4) TRAIN VITERBI CROSS-ENTROPY 5.63713 PERPLEXITY 49.7673

Model3 Viterbi Iteration : 4 took: 0 seconds

---------------------
Model3: Iteration 5
#centers(pre/hillclimbed/real): 1 1 1  #al: 55.0789 #alsophisticatedcountcollection: 0 #hcsteps: 1.76561
#peggingImprovements: 0
A/D table contains 639 parameters.
A/D table contains 1098 parameters.
NTable contains 560 parameter.
p0_count is 5173.68 and p1 is 635.158; p0 is 0.98 p1: 0.02
Model3: TRAIN CROSS-ENTROPY 5.5049 PERPLEXITY 45.4089
Model3: (5) TRAIN VITERBI CROSS-ENTROPY 5.61202 PERPLEXITY 48.9086

Model3 Viterbi Iteration : 5 took: 0 seconds

---------------------
T3To4: Iteration 6
#centers(pre/hillclimbed/real): 1 1 1  #al: 55.053 #alsophisticatedcountcollection: 11.9105 #hcsteps: 1.75383
#peggingImprovements: 0
D4 table contains 205842 parameters.
A/D table contains 639 parameters.
A/D table contains 1098 parameters.
NTable contains 560 parameter.
p0_count is 5194.06 and p1 is 624.97; p0 is 0.98 p1: 0.02
T3To4: TRAIN CROSS-ENTROPY 5.49145 PERPLEXITY 44.9874
T3To4: (6) TRAIN VITERBI CROSS-ENTROPY 5.59324 PERPLEXITY 48.2761

T3To4 Viterbi Iteration : 6 took: 0 seconds

---------------------
Model4: Iteration 7
#centers(pre/hillclimbed/real): 1 1 1  #al: 54.8787 #alsophisticatedcountcollection: 9.11307 #hcsteps: 1.649
#peggingImprovements: 0
D4 table contains 205842 parameters.
A/D table contains 639 parameters.
A/D table contains 1098 parameters.
NTable contains 560 parameter.
p0_count is 5284.27 and p1 is 579.863; p0 is 0.98 p1: 0.02
Model4: TRAIN CROSS-ENTROPY 4.98334 PERPLEXITY 31.6327
Model4: (7) TRAIN VITERBI CROSS-ENTROPY 5.05065 PERPLEXITY 33.1433

Model4 Viterbi Iteration : 7 took: 0 seconds

---------------------
Model4: Iteration 8
#centers(pre/hillclimbed/real): 1 1 1  #al: 54.7939 #alsophisticatedcountcollection: 7.62073 #hcsteps: 1.60895
#peggingImprovements: 0
D4 table contains 206045 parameters.
A/D table contains 639 parameters.
A/D table contains 1098 parameters.
NTable contains 560 parameter.
p0_count is 5341.66 and p1 is 551.172; p0 is 0.98 p1: 0.02
Model4: TRAIN CROSS-ENTROPY 4.82963 PERPLEXITY 28.4357
Model4: (8) TRAIN VITERBI CROSS-ENTROPY 4.8783 PERPLEXITY 29.4113

Model4 Viterbi Iteration : 8 took: 0 seconds

---------------------
Model4: Iteration 9
#centers(pre/hillclimbed/real): 1 1 1  #al: 54.7444 #alsophisticatedcountcollection: 6.95053 #hcsteps: 1.59011
#peggingImprovements: 0
D4 table contains 206045 parameters.
A/D table contains 639 parameters.
A/D table contains 1098 parameters.
NTable contains 560 parameter.
p0_count is 5390.88 and p1 is 526.561; p0 is 0.98 p1: 0.02
Model4: TRAIN CROSS-ENTROPY 4.77058 PERPLEXITY 27.2954
Model4: (9) TRAIN VITERBI CROSS-ENTROPY 4.81325 PERPLEXITY 28.1147

Model4 Viterbi Iteration : 9 took: 0 seconds

---------------------
Model4: Iteration 10
#centers(pre/hillclimbed/real): 1 1 1  #al: 54.6596 #alsophisticatedcountcollection: 6.58304 #hcsteps: 1.59011
#peggingImprovements: 0
D4 table contains 206248 parameters.
A/D table contains 639 parameters.
A/D table contains 1098 parameters.
NTable contains 560 parameter.
p0_count is 5429.19 and p1 is 507.405; p0 is 0.98 p1: 0.02
Model4: TRAIN CROSS-ENTROPY 4.74076 PERPLEXITY 26.7369
Model4: (10) TRAIN VITERBI CROSS-ENTROPY 4.78049 PERPLEXITY 27.4834
Dumping alignment table (a) to file:fereshte.a3.final
Dumping distortion table (d) to file:fereshte.d3.final
Dumping nTable to: fereshte.n3.final

Model4 Viterbi Iteration : 10 took: 0 seconds
H3333344444 Training Finished at: Tue Aug 25 15:48:25 2015


Entire Viterbi H3333344444 Training took: 0 seconds
==========================================================
writing Final tables to Disk1
writing Final tables to Disk 
Dumping the t table inverse to file: fereshte.ti.final
writing Final tables to Disk 
Dumping the t table inverse to file: fereshte.actual.ti.final
Writing PERPLEXITY report to: fereshte.perp
f youWriting source vocabulary list to : fereshte.trn.src.vcb
Writing source vocabulary list to : fereshte.trn.trg.vcb
Writing source vocabulary list to : fereshte.tst.src.vcb
Writing source vocabulary list to : fereshte.tst.trg.vcb
writing decoder configuration file to fereshte.Decoder.config

Entire Training took: 0 seconds
Program Finished at: Tue Aug 25 15:48:25 2015

==========================================================
